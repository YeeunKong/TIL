# **머신러닝**
--------------------------------------------------------------------

지도학습) 회귀와 분류


회귀: 
- 연속적인 값의 범위 내에서 예측 값이 나오는 경우(결과가 연속값)
- 데이터의 특징과 라벨 사이의 관계를 추정하는 방법으로, 모델을 통하여 임의의 특징에 대해 라벨을 예측할 수 있음


분류: 
- 둘 혹은 여러 개의 선택지 중에 답을 선택하는 경우(결과가 이산값)

cf) 분류와 군집화

- 분류: 소속 집단의 정보를 알고 있고, 레이블이 있는 상태에서 비슷한 집단으로 묶음(지도 학습)
- 군집화: 소속 집단의 정보가 없고, 레이블도 없는 상태에서 비슷한 집단으로 묶음(비지도 학습)

--------------------------------------------------------------------
## **분류**
### **1. k-NN 알고리즘**
- k-NN은 k-Nearest Neighbor의 약자로, 특징 공간에 분포하는 데이터에 대하여 k개의 가장 가까운 이웃을 살펴보고 다수결 방식으로 데이터의 레이블을 할당하는 분류 방식이다.
- 예를 들어 k=3일 때는 가장 가까운 3개의 이웃을 보고, k=5일 때는 가장 가까운 5개의 이웃을 보고 다수결로 레이블을 할당한다.
- k가 짝수이고, 레이블의 개수가 같다면, 이웃간의 거리에 가중치를 부여하여 클래스를 분류한다.


- 단점: k-NN 알고리즘은 특징 공간에 있는 모든 데이터에 대한 정보가 필요하다. 데이터의 클래스, 특징의 요소들의 개수가 많으면, 많은 메모리 공간과 계산 시간이 필요하다. 
- 장점: 알고리즘이 매우 단순하고 직관적이며, 사전 학습이나 특별한 준비 시간이 필요 없다.


### **2. 앙상블 기법**
- 모델이 어느 수준의 성능에 도달하면 분류기를 개선하는 일이 매우 어렵다.
- 따라서 분류기를 여러개 믹스해 사용하는 다양한 기법을 통해 전체 모델 성능을 높이려는 앙상블 기법이 고안되었다.

### 1) Voting
- 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식이다.
- 서로 다른 알고리즘을 여러 개 결합하여 사용하는 기법으로 하드 보팅과 소프트 보팅으로 나눌 수 있다.
- 하드 보팅: 다수의 분류기가 예측한 결과값을 최종 결과로 선정(다수결)
- 소프트 보팅: 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 값을 최종 결과로 선정(확률)


### 2) Bagging
- 데이터 샘플링을 통해 모델을 학습시키고 결과를 집계한다.
- 모두 같은 유형의 알고리즘 분류기를 사용하며, 데이터 분할 시 중복을 허용한다.
- 즉, 같은 분류기에 데이터를 샘플링하는 방법만 계속 바꾸어 최적의 결과를 내는 것이다.

### 3) Boosting
- 여러 개의 분류기가 순차적으로 학습을 수행하는데, 이전 분류기에서 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 가중치를 부여하면서 학습과 예측을 진행한다.
- 이처럼 순차적으로 계속 분류기에 가중치를 부스팅하며 학습을 진행하기 때문에 부스팅 방식이라고 부른다.
- 다른 앙상블 기법보다 예측 성능이 우수하다.
- 그러나 속도가 느리고 과적합이 발생할 가능성이 존재한다.

--------------------------------------------------------------------

## **군집화**
### **1. k-means(평균) 알고리즘**
- 데이터를 2개의 그룹으로 나누는 k-평균 알고리즘으로, sklearn에서 제공하는 cluster모듈을 통해 사용할 수 있다.
(from sklearn import cluster)

- k-평균 알고리즘은 원리가 단순하고 직관적이며 성능이 좋은 군집화 알고리즘이다. 그러나 사전에 군집의 개수 k를 지정해야 하는 단점이 존재한다. (군집을 몇개로 나누는 것이 최적인지 알아야 함-> 엘보우 기법 이용)

- k-평균 알고리즘은 데이터 분포에 대한 사전 지식이 없어도 사용할 수 있다. 

cf) k-NN과 k-means 알고리즘을 비교
- k-NN은 지도 학습이므로, 정답인 레이블이 존재하지만, k-means 알고리즘은 비지도 학습이므로 훈련에 사용하는 데이터에 정답이 알려져 있지 않다.
- k-NN에서 k는 이웃한 데이터의 개수이지만, k-평균 알고리즘에서 k는 군집의 개수이다.


### **2. 엘보우 기법**
- 엘보우 기법은 최적의 군집 개수를 선택하는 기법이다.
- 클러스터 개수를 늘렸을 때 중심점과 평균 거리가 더 이상 많이 감소하지 않는 경우의 k를 선택하는 방법이다.

### **3. 실루엣 분석**
- 실루엣 분석은 실루엣 계수를 기반으로 개별 데이터가 가지는 군집화 지표이다.
- 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화 되어있는지, 다른 군집의 데이터와는 얼마나 멀리 분리되어 있는지를 나타내는 지표이다.




--------------------------------------------------------------------
## 성능평가 지표
- True Positive(TP): 실제 참을 참으로 예측
- False Positive(FP): 실제 거짓을 참으로 예측
- False Negative(FN): 실제 참을 거짓으로 예측
- True Negative(TN): 실제 거짓을 거짓으로 예측


- 정밀도: 모델이 True라고 예측한 것 중에서 실제 True인 것의 비율
- 재현율: 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율
- 정확도: 전체 예측한 데이터 중에서 정답을 맞춘 것에 대한 비율

- 그러나 정확도가 적절한 성능 평가의 기준이 되지 않는 경우도 있고, 재현율과 정밀도는 관심있는 척도가 다르기 때문에 하나의 척도만을 측정 방법으로 사용할 경우 왜곡이 발생할 수 있다.
--> 두 지표의 조화평균으로 계산된 F1 점수를 사용한다.
