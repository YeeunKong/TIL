## 머신러닝의 다양한 기법

-----------------------------------------------------------
### **다항 회귀**
### 다중 선형 회귀
- 여러 개의 종속변수(특징)과 하나의 독립변수(라벨) 간의 관계를 분석한다.
- 학습 결과 선형적으로 데이터가 분포되어 있다.

  $f(x, y, z) = w_0 + w_1x + w_2y + w_3z$


### 다항 회귀
- 실제 데이터는 비선형적으로 분포되어 있을 수 있고, 이를 때 다항 회귀를 사용하여 분석해야 한다.
- 다항식의 차수에 따라 과소 또는 과대 적합의 문제가 발생할 수 있다.

  $f(x) = ax^2 + bx + c$

cf) 회귀 이름 정리
- 독립변수(x) 개수-> 1개:단순, 2개 이상: 다중
- 종속변수(y) 개수-> 1개:x, 2개 이상: 다변량
- 모델의 차수-> 1차: 선형, 2개 이상: 비선형
  
  예) 독립변수(x)가 2개 이상이고, 종속변수(y)가 하나이며 모델이 1차이면 -> '다중 선형 회귀분석'
-----------------------------------------------------------
### **로지스틱 회귀**
- 로지스틱 회귀는 특정 조건(환경)이 주어졌을 때, 사건의 발생여부를 반환한다.(0 or 1)
- 임의의 사건에 대해서, X라는 조건이 주어졌을 때, 결과가 O/X로 나뉘는 문제를 예측하는 것이 로지스틱 회귀의 목적이다.
-----------------------------------------------------------
### **결정 트리**
- 결정 트리는 귀납 추론을 위해 사용되는 실용적인 방법이다.
- 데이터들을 트리 구조의 루트에서 시작하여 차례로 중간 노드들을 거쳐 단말 노드에 배정하는 기능을 수행한다.

- 머신러닝에서 결정 트리는 데이터가 주어졌을 때, 데이터의 특징을 보고 분류에 적절한 트리를 찾아내는 것을 목표로 한다.
- 데이터에서 어떤 속성이 가장 중요한지 판단하기 위해 '정보 이득'이라는 개념을 사용한다. 
- 결정 트리 알고리즘으로는 ID3와 CART 알고리즘이 있는데, ID3는 정보 이득, CART 알고리즘은 불순도 개념을 사용한다.

1) ID3 알고리즘

2) CART(Classification And Regression Tree) 알고리즘
- 정보 이득 개념(ID3 알고리즘)은 다소 복잡한 수식과 계산을 요구하므로 이를 피하기 위해 엔트로피의 대체 척도인 **지니 불순도**를 활용한다.
- CART 알고리즘은 현재 데이터가 섞여 있는 노드를 두 개의 노드로 나눌 때 어떤 속성 A와 해당 속성의 어떤 값 a를 기준으로 분류할 것인지를 찾는다.
- 분류한 뒤 얻는 두 노드를 L과 R이라고 하고, 각각의 원소 개수를 $m_L, m_R$ 이라고 하고, 분류하기 전 노드의 원소 개수를 m이라고 하자.
- 알고리즘이 하는 일은 불순도 $G_L$과 $G_R$로 결정되는 다음 비용 함수 $J(A, a)$가 최소가 되게 하는 $A$와 $a$를 선택하는 것

  $argmin J(A, a) = argmin(\frac{m_L}{m}G_L + \frac{m_R}{m}G_R)$


### cf) 엔트로피와 정보 이득, 정보량과 불순도
**엔트로피**
- 엔트로피는 정보량을 측정하기 위해 고안된 것으로, 이 값이 크면 많은 정보가 담겨 있다는 것을 뜻한다.
- 전체 데이터 표본 S에서 차지하는 양성 데이터의 비율을 $p^+$, 음성인 데이터 비율을 $p^-$라고 하면 이 데이터 표본의 엔트로피는 다음과 같다. 

  $H(S) = -p^+lgp^+ - p^-lgp^-$

- 엔트로피가 가장 높은 경우는 두 부류가 균등하게 섞여 있는 경우로, 각각 비율이 1/2인 경우이다.

**정보이득**
- 정보 이득은 특정한 속성이 원하는 분류 방식에 부합하게 데이터를 나누는지 측정할 수 있는 척도이다.
- 정보 이득은 특정한 속성에 따라 데이터를 나누었을 때, 줄어드는 엔트로피로 정의한다.

  (엔트로피 높다 = 정보량이 많다 = 많은 정보가 담겨 있다 = 정보가 균등하게 있다 = 분류가 잘 되지 않았다 = 정보 이득이 낮다)

  (엔트로피 낮다 = 정보량이 적다 = 적은 정보가 담겨 있다 = 정보가 한쪽으로 치우쳐 있다 = 분류가 잘 되었다 = 정보 이득이 높다)

- 어떤 속성 A가 가지는 모든 값들의 집합을 A라고 할 때, 데이터 표본 S를 나누어서 얻는 정보 이득 G(S, A)는 다음과 같다.

  $G(S, A) = H(S) - \displaystyle\sum_{a\in A}{\frac{\lvert S_a \rvert}  {\lvert S \rvert} H(S_a)}$



**불순도**
- 분할의 기준을 엔트로피가 아닌 데이터의 순도, 불순도 개념으로 설명할 수도 있다.
- 순도가 높다 = 한 가지 데이터가 많다 = 분류가 잘 되었다
- 하나의 그룹 내에 섞여 있는 n개 종류의 레이블이 있고, 레이블이 $i$인 객체의 비율이 $p_i$라고 할 때 다음과 같은 값을 가진다.

  $G = 1- \sum _{i=1} ^{n}{p_i^2}$

-----------------------------------------------------------
### **SVM(Support vector machine)**
- SVM은 두 데이터 그룹을 나누는 초평면을 찾으면서 이 폭(**마진**)이 가장 넓은 것을 찾는 방법이다.

- **하드 마진**: 마진 내에 어떠한 데이터도 들어오지 않을 경우 하드 마진이라고 함. 

  - 실제 데이터는 하드 마진을 사용할 경우 분류가 불가능할 수 있다.
  - 하드 마진을 그릴 수 있더라고 과적합이 될 가능성이 있다.
  - 어떤 데이터는 잡음에 가까워 무시하는 것이 좋을 수도 있다. (과적합될 수 있다)


- **소프트 마진**: 일부 데이터가 마진 내에 들어오도록 허용하면서 분리 평면을 찾을 수 있는 경우를 소프트 마진이라고 함.

  - 소프트 마진을 사용하는 것이 모델 정착화의 일종이다.
  - 소프트 마진을 사용할 때는 마진 내에 들어갈 수 있는 데이터의 수를 제어하는데, 이 값을 제어하는 변수를 '슬랙'이라고 한다.
  - 슬랙 변수는 각 데이터 인스턴스마다 정의되므로, m개의 데이터 인스턴스가 있으면 각각에 대해 $\zeta^{(i)}$가 존재한다.
  - 슬랙 변수는 각각의 데이터가 [-1, 1] 사이의 범위를 갖는 마진 안으로 들어갈 수 있는 정도를 의미한다. (하드 마진은 $\zeta^{(i)} = 0$)

- **다항 특징 변환을 통한 비선형 SVM**
  - 선형 SVM으로는 데이터를 분류하지 못하는 경우가 있다.
  - 이러한 경우 다항 특징 변환을 하면 새로운 특징 공간으로 확장되는데, 이 공간에서는 분리 평면이 존재한다. (원래의 입력 공간에서 보면 비선형 곡면을 통한 분류와 같은 효과) 